{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIb1ABIbNzIq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "import torch\n",
        "import pickle as pkl\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "emrroaqoOIAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
        "bert = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")"
      ],
      "metadata": {
        "id": "60Q8WdCDdq-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujVpZO_bhqKc",
        "outputId": "7d026c3c-e17b-4c9a-a01c-a61961d883a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт данных\n"
      ],
      "metadata": {
        "id": "IjhBHkmZOYi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = None\n",
        "val_df = None\n",
        "test_df = None"
      ],
      "metadata": {
        "id": "eOVAuLlJj99X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разобьем выборки на текст и таргет\n",
        "\n",
        "train_text = train_df['text'].astype('str')\n",
        "train_labels = train_df['target']\n",
        "val_text = val_df['text'].astype('str')\n",
        "val_labels = val_df['target']\n",
        "test_text = test_df['text'].astype('str')\n",
        "test_labels = test_df['target']"
      ],
      "metadata": {
        "id": "F-Bft9IvOYw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Следующий код выведет график длин предложений. Он поможет определить оптимальную длину последовательности токенов, чтобы избежать разреженных векторов."
      ],
      "metadata": {
        "id": "aIwl957UPW2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(str(i).split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 50)"
      ],
      "metadata": {
        "id": "izteexviOejd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизируем текста, передадим в тензоры и загрузим в функцию DataLoader, которая будет по частям подавать наши данные для обучения и валидации в модель"
      ],
      "metadata": {
        "id": "Bgc7TJlFPctI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.values,\n",
        "    max_length = 50,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.values,\n",
        "    max_length = 50,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.values,\n",
        "    max_length = 50,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.values)\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.values)\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.values)\n",
        "batch_size = 8\n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "\n",
        "val_data =  TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "yH5wGCimO2CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сам BERT обучать не будем. Допишем к его выходу свои слои, которые и будем обучать на классификацию."
      ],
      "metadata": {
        "id": "NucT3hARQGCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask = mask, return_dict = False)\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JU0wuNFrO3kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Объявим модель, загрузим ее в GPU. Импортируем оптимизатор.\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "model = model.to(device)\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr= 1e-3)"
      ],
      "metadata": {
        "id": "3Dgogc4KO-jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Для борьбы с дисбалансом классов используем следующий подход"
      ],
      "metadata": {
        "id": "G-VMwtvLQ2df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "V8IPhhifPB2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.tensor(class_weights, dtype = torch.float)\n",
        "weights = weights.to(device)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "wJu904cjjc0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функция для обучения модели"
      ],
      "metadata": {
        "id": "ulzQwJRcQ5fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id,mask,labels = batch\n",
        "        model.zero_grad()\n",
        "        preds = model(sent_id, mask)\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        total_preds.append(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    total_preds = np.concatenate(total_preds, axis = 0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "RVlIHH-lQ9VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функция валидации"
      ],
      "metadata": {
        "id": "3RUtxmoMQ983"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss, total_accuracy = 0,0\n",
        "    total_preds = []\n",
        "\n",
        "    for step, batch in tqdm(enumerate(val_dataloader), total = len(val_dataloader)):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(sent_id, mask)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss = total_loss + loss.item()\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    total_preds = np.concatenate(total_preds, axis = 0)"
      ],
      "metadata": {
        "id": "cEznWxeRRCCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение"
      ],
      "metadata": {
        "id": "Y0A7cQxxRCv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('\\n Epoch{:} / {:}'.format(epoch+1, epochs))\n",
        "\n",
        "    train_loss, _ = train()\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    print(f'\\nTraining loss: {train_loss:.3f}')\n",
        "    print(f'Validation loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "GG6TR4SyRHBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим лучшие веса для модели"
      ],
      "metadata": {
        "id": "sQHi7tTLRSQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "y7vvV3yQRU-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Если не хватило видеопамяти. Используем костыль. Разобьем тестовые данные на части и будем отправлять на предсказание по частям"
      ],
      "metadata": {
        "id": "pcDBXPncSo-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "list_seq = np.array_split(test_seq, 50)\n",
        "list_mask = np.array_split(test_mask, 50)\n",
        "\n",
        "\n",
        "predictions = []\n",
        "for num, elem in enumerate(list_seq):\n",
        "    with torch.no_grad():\n",
        "        preds = model(elem.to(device), list_mask[num].to(device))\n",
        "        predictions.append(preds.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "zlRmMR59S0N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Преобразуем полученные предсказания в один список, нормализуем данные и запишем в новый столбик датафрейма"
      ],
      "metadata": {
        "id": "8CN1j5IuS2ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_preds = [item[1] for sublist in predictions for item in sublist]\n",
        "flat_preds = (flat_preds - min(flat_preds)) / (max(flat_preds) - min(flat_preds))\n",
        "test_df['confidence'] = flat_preds"
      ],
      "metadata": {
        "id": "aYN9GIjMS2TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['pred'] = test_df['confidence'].apply(lambda x: 1 if x>0.92 else 0)\n",
        "\n",
        "print(classification_report(test_df['target'], test_df['pred']))"
      ],
      "metadata": {
        "id": "NEovhLzyS9p2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}